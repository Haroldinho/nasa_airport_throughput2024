{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00cbcadf-0139-4f5d-aaa1-082885362b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploring the datasets\n",
    "- FUSER\n",
    "- METAR\n",
    "- Terminal Aerodrome Forecast (TAF)\n",
    "- Convective Weather Avoidance Model (CWAM)\n",
    "\n",
    "Save the result of the initial exploration into delta live tables for quick access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ece9833-ef4f-43f5-9c87-be2504502d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Locate files in mounted DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b70d8fbf-cd4c-4221-bdfc-fdbfa8a47abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install autoviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2ff4f63-c6c1-41c5-b0a9-0b5f2b76926e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q handyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5039e72e-7bbc-466b-8d40-a530fc341b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed37638-8d13-412d-9fd0-6be6d120a547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/mnt/nasa_challenge/1-raw-unzipped-files/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df2d90dd-8a4e-4cd6-b772-11c8dd818008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741c0fbb-06f0-4b72-a519-94aa810a580f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5779dca-df79-4734-9a58-5a4b79b22180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "submission_df = spark.read.csv(\"dbfs:/mnt/nasa_challenge/1-raw-unzipped-files/submission_format.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b05968-1574-4d9e-9b95-c0c1b0c23762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "submission_pd_df = submission_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "631193b2-a1ab-4a0b-99a1-4f1b02ca310a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "submission_pd_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8285ed-a369-44a9-a0bd-46e08d69605e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "submission_pd_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec467f06-0ace-453d-aedc-d1f0a091018a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Example KMEM_221129_0500_135\t  99\n",
    "Where KMEM is the ICAO code for the airport, 221129 is the date (YYMMDD), 0500 is the time of the prediction (HHMM) (24 hour format) and 15, 30,.. are the buckets into the future (from 1:00 to 1:15, from 1:15 to 1:30, etc.).\n",
    "The order of the predictions should be the same as in the submission_format.csv file. Failing to follow this format will result in an error or lower score.\n",
    "\n",
    "The evaluation metric is based on Root Mean Squared Error (RMSE). Specifically, it is calculated as exp(-RMSE/K), where K is a normalization factor of 10, and exp is the exponential function in order for it to be a number between 0.0 and 1.0, with 1.0 representing a perfect score (RMSE = 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bae0de4-2873-4851-a602-53f9f3ae046b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# all the throughputs are set. to 99\n",
    "submission_pd_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41589adb-0ba6-43a3-bb70-98a036ad44ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- ID is the time format of the prediction\n",
    "- value is the throughput in the number of flights per the time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98703f6b-f66a-4c62-92ad-f4797c9ce39d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_info(entry):\n",
    "    pattern = r\"([A-Z]{4})_(\\d{2})(\\d{2})(\\d{2})_(\\d{2})(\\d{2})_(\\d{2,3})\"\n",
    "    match = re.match(pattern, entry)\n",
    "    if match:\n",
    "        city, year, month, day, hour, minute, buc = match.groups()\n",
    "        return {\n",
    "            \"city\": city,\n",
    "            \"year\": \"20{}\".format(year),\n",
    "            \"month\": month,\n",
    "            \"day\": day,\n",
    "            \"hour\": hour,\n",
    "            \"minute\": minute,\n",
    "            \"buc\": buc\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_entry(entry, feature_name):\n",
    "    extracted_info = extract_info(entry)    \n",
    "    if extracted_info:\n",
    "        return extracted_info[feature_name]\n",
    "    else:\n",
    "        return extracted_info\n",
    "    \n",
    "\n",
    "submission_pd_df['city'] = submission_pd_df['ID'].apply(extract_entry, feature_name=\"city\")\n",
    "submission_pd_df['year'] = submission_pd_df['ID'].apply(extract_entry, feature_name=\"year\")\n",
    "submission_pd_df['month'] = submission_pd_df['ID'].apply(extract_entry, feature_name=\"month\")\n",
    "submission_pd_df['day'] = submission_pd_df['ID'].apply(extract_entry, feature_name=\"day\")\n",
    "submission_pd_df['hour'] = submission_pd_df['ID'].apply(extract_entry, feature_name=\"hour\")\n",
    "submission_pd_df['minute'] = submission_pd_df['ID'].apply(extract_entry, feature_name=\"minute\")\n",
    "submission_pd_df['min_bucket'] = submission_pd_df['ID'].apply(extract_entry, feature_name=\"buc\").astype(int)\n",
    "submission_pd_df['prediction_datetime'] = pd.to_datetime(submission_pd_df[['year', 'month', 'day', 'hour', 'minute']])\n",
    "submission_pd_df['extracted_info'] = submission_pd_df['ID'].apply(extract_info)\n",
    "\n",
    "display(submission_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7c2757d-fc86-4006-8d55-716d2d42ec49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get the unique cities\n",
    "cities = submission_pd_df['city'].unique()\n",
    "# get the unique years\n",
    "years = submission_pd_df['year'].unique()\n",
    "# get start and end prediction dates in the data\n",
    "first_pred_datetime = submission_pd_df['prediction_datetime'].min()\n",
    "last_pred_datetime = submission_pd_df['prediction_datetime'].max()\n",
    "# get the longest lookahead bucket\n",
    "max_lookahead_bucket_in_min = submission_pd_df['min_bucket'].max()\n",
    "min_lookahead_bucket_in_min = submission_pd_df['min_bucket'].min()\n",
    "print(\"The cities in the dataset are: {}\".format(cities))\n",
    "print(\"The years in the dataset are: {}\".format(years))\n",
    "print(\"The predictions go from {} to {}\".format(first_pred_datetime, last_pred_datetime))\n",
    "print(\"We are looking {}-{} minutes into the future\".format(min_lookahead_bucket_in_min, max_lookahead_bucket_in_min))\n",
    "print(\"This is 15 minutes to 3 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36dcd401-4912-45c6-964f-df325cf5245f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## FUSER\n",
    " The fuser data is airport specific:\n",
    " - KATL (Atlanta)\n",
    " - KCLT (Charlotte)\n",
    " - KDEN (Denver)\n",
    " - KDFW (Dallas Fort Worth)\n",
    " - KJFK (New York JFK)\n",
    " - KMEM (Memphis)\n",
    " - KORD (Chicago O'Hare)\n",
    " - KPHX (Phoenix)\n",
    " - KSEA (Seattle)\n",
    "\n",
    " The data also contains the test set *FUSER_test*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc939158-7002-4543-aae8-d68cc10c0925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d83f911-1b37-4218-8cb5-6092f858c3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/mnt/nasa_challenge/1-raw-unzipped-files/FUSER_train_KATL/KATL/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4e20ca-27a3-4d27-a874-ceba628ff456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "FUSER data can be further broken down into: \n",
    "- Configs Data Set (D-ATIS Data) -- configs_data_set\n",
    "- Runways Data Set (Arrival Departure Detection) TARGET. --runways_data_set\n",
    "- First Position Data Set -- first_position_data_set\n",
    "- TBFM Data Set (Time-Based Flow Management Data) -- TBFM_data_set\n",
    "- TFM Track Data Set (Traffic Flow Management Data) -- TFM_track_data_set\n",
    "- ETD Data Set (Estimated Time of Departure) -- ETD_data_set\n",
    "- LAMP Data Set (Local Aviation MOS Program Data) -- LAMP_data_set\n",
    "- MFS Data Set (FAA SWIM Feeds) -- MFS_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf514fbf-773b-4dfd-93a1-32b00cbb623d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fuser_data_types = [\n",
    "    'configs_data_set',\n",
    "    'runways_data_set',\n",
    "    'first_position_data_set',\n",
    "    'TBFM_data_set',\n",
    "    'TFM_track_data_set',\n",
    "    'ETD_data_set',\n",
    "    'LAMP_data_set',\n",
    "    'MFS_data_set',\n",
    "]\n",
    "list_of_airport_centers = ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', 'KMEM', 'KORD', \n",
    "                           'KPHX', 'KSEA'] \n",
    "fuser_df = {airport_center:{} for airport_center in list_of_airport_centers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c10473-d3ee-48f2-90c0-ce34e84f7024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a dictionary of dataframe for each airport and each data table in FUSER\n",
    "for airport_center in list_of_airport_centers:\n",
    "    for fuser_data_type in fuser_data_types:\n",
    "        fuser_df[airport_center][fuser_data_type] = spark.read.csv(f\"dbfs:/mnt/nasa_challenge/1-raw-unzipped-files/FUSER_train_{airport_center}/{airport_center}/*.{fuser_data_type}.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f4c810-c361-4ddc-9628-49fccb8ffaa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for fuser_data_type, df in fuser_df[list_of_airport_centers[0]].items():\n",
    "    num_rows = df.count()\n",
    "    column_names = df.columns\n",
    "    # num_distinct_dates = df.select(\"date_column\").distinct().count()  # Replace \"date_column\" with the actual date column name\n",
    "    print(f\"Data Type: {fuser_data_type}\")\n",
    "    print(f\"Number of Rows: {num_rows}\")\n",
    "    print(f\"Column Names: {column_names}\")\n",
    "    # print(f\"Number of Distinct Dates: {num_distinct_dates}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477a15e5-5028-4a0a-80f5-d522e1996f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fields for configs_data_set\n",
    "fuser_df[list_of_airport_centers[0]]['configs_data_set'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440a3e9f-c2f2-4330-82b6-34f1e1c6abbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for fuser_data_type, df in fuser_df[list_of_airport_centers[0]].items():\n",
    "    df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c456207d-a51e-4d02-a37d-a0bd786aee38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for fuser_data_type, df in fuser_df[list_of_airport_centers[0]].items():\n",
    "    print('Data overview')\n",
    "    print(fuser_data_type)\n",
    "    df.printSchema()\n",
    "    print('Columns overview')\n",
    "    pd.DataFrame(df.dtypes, columns = ['Column Name','Data type'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7e2d1f-bb2c-4e8a-87ea-920931f6c904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmdXNlcl9kZltsaXN0X29mX2FpcnBvcnRfY2VudGVyc1swXV1bJ2NvbmZpZ3NfZGF0YV9zZXQnXSk=\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "ed5746d7-600c-4385-868d-42ff71898ae2",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 13.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# atlanta configs_data_set\n",
    "display(fuser_df[list_of_airport_centers[0]]['configs_data_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b994fa2-5f19-46fb-a048-27d34044d2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmdXNlcl9kZltsaXN0X29mX2FpcnBvcnRfY2VudGVyc1swXV1bJ3J1bndheXNfZGF0YV9zZXQnXSk=\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "add64e92-697f-4f78-812e-f18ab74bd176",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 13.25,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# atlanta runways_data_set\n",
    "display(fuser_df[list_of_airport_centers[0]]['runways_data_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8520600-3fe7-4437-986c-92b877386a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmdXNlcl9kZltsaXN0X29mX2FpcnBvcnRfY2VudGVyc1swXV1bJ2ZpcnN0X3Bvc2l0aW9uX2RhdGFfc2V0J10p\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "a71a00e7-f537-49df-854f-37b062efcb05",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 13.375,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(fuser_df[list_of_airport_centers[0]]['first_position_data_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a39bb86-53f1-4b86-bcd0-396a74f32624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmdXNlcl9kZltsaXN0X29mX2FpcnBvcnRfY2VudGVyc1swXV1bJ1RCRk1fZGF0YV9zZXQnXSk=\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "b2472d59-9dcc-438a-8ef0-a90c3c8f1fc6",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 13.4375,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(fuser_df[list_of_airport_centers[0]]['TBFM_data_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b98f5f-729e-44ef-b3a4-4e2299865373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmdXNlcl9kZltsaXN0X29mX2FpcnBvcnRfY2VudGVyc1swXV1bJ1RGTV90cmFja19kYXRhX3NldCddKQ==\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "c96a6b4b-39ab-48e6-981f-15e2f22b19ed",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 13.46875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(fuser_df[list_of_airport_centers[0]]['TFM_track_data_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccec6b0a-8e5f-4781-b0de-4cc05430ff2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmdXNlcl9kZltsaXN0X29mX2FpcnBvcnRfY2VudGVyc1swXV1bJ0VURF9kYXRhX3NldCddKQ==\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "4ce0d2f2-b49e-4bd7-9e59-5f89aa6ddfbb",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 13.484375,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(fuser_df[list_of_airport_centers[0]]['ETD_data_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eebdd57-2c7f-4037-96e6-f98143a36dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmdXNlcl9kZltsaXN0X29mX2FpcnBvcnRfY2VudGVyc1swXV1bJ0xBTVBfZGF0YV9zZXQnXSk=\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "23556271-01a8-4cb2-91ac-63273e6b7aac",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 13.4921875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(fuser_df[list_of_airport_centers[0]]['LAMP_data_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa9cf13-35d0-45cd-819a-60ad6e6af829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmdXNlcl9kZltsaXN0X29mX2FpcnBvcnRfY2VudGVyc1swXV1bJ01GU19kYXRhX3NldCddKQ==\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "d56ce464-ab31-4e49-ae56-bcd040e86c75",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 13.49609375,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(fuser_df[list_of_airport_centers[0]]['MFS_data_set'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033df42e-405e-48ff-b23e-29923599af89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Store data as delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9babdc32-2577-405b-9889-66d5a0498e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fuser_df['KCLT']['configs_data_set'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8726f61-86d1-4687-b46b-dbfe0366509d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fuser_df['KATL']['configs_data_set'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d05ae63f-ff15-4207-b801-478922207490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "for airport_center in list_of_airport_centers:\n",
    "    print(f\"Processing {airport_center}\")\n",
    "    for fuser_data_type in fuser_data_types:\n",
    "        df = fuser_df[airport_center][fuser_data_type]\n",
    "        if fuser_data_type=='configs_data_set':\n",
    "            df = df.withColumn(\"invalid_departure_runways\", col(\"invalid_departure_runways\").cast(\"double\"))\n",
    "            df = df.withColumn(\"invalid_departure_runways\", col(\"invalid_arrival_runways\").cast(\"double\"))\n",
    "        config_clt_df = config_clt_df.withColumn(\"invalid_arrival_runways\", col(\"invalid_arrival_runways\").cast(\"double\"))\n",
    "        table_name = f\"train_{fuser_data_type}_table\"\n",
    "        print(f\"\\t Processing {fuser_data_type}\")\n",
    "        df.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"append\").saveAsTable(table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0-explore_my_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
